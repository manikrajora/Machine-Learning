{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import *\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "epsilon = 1\n",
    "Gamma = 0.99\n",
    "max_eps = epsilon\n",
    "min_eps = 0.1\n",
    "LAMBDA = 0.001\n",
    "mem_allowed = 100000\n",
    "mem_all = []\n",
    "batch_size = 100\n",
    "state_num = 8\n",
    "action_num = 4\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_Greedy(epsilon,v):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(0,env.action_space.n)\n",
    "    else:\n",
    "        return np.argmax(v)\n",
    "    return k    \n",
    "\n",
    "def eps_reduce(epsilon,min_eps,max_eps,step):\n",
    "    return min_eps + (max_eps - min_eps) * math.exp(-LAMBDA*step)\n",
    "\n",
    "def predict_2(model,data):\n",
    "    return model.predict(data.reshape(1,state_num)).flatten()\n",
    "\n",
    "\n",
    "def experience(exp_all,sample):\n",
    "    return exp_all.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manik.Rajora\\AppData\\Local\\Continuum\\anaconda2\\envs\\Py36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=8, units=120)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Manik.Rajora\\AppData\\Local\\Continuum\\anaconda2\\envs\\Py36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=60)`\n",
      "  \n",
      "C:\\Users\\Manik.Rajora\\AppData\\Local\\Continuum\\anaconda2\\envs\\Py36\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=4)`\n",
      "  import sys\n",
      "C:\\Users\\Manik.Rajora\\AppData\\Local\\Continuum\\anaconda2\\envs\\Py36\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for episode 0 is -232.43461347077667\n",
      "Reward for episode 1 is -274.0682893238844\n",
      "Reward for episode 2 is -159.8533428077896\n",
      "Reward for episode 3 is -131.94620598972884\n",
      "Reward for episode 4 is -96.5090426175518\n",
      "Reward for episode 5 is -457.52863069343346\n",
      "Reward for episode 6 is -239.5101957434258\n",
      "Reward for episode 7 is -125.03536084271764\n",
      "Reward for episode 8 is -129.06583763086576\n",
      "Reward for episode 9 is -72.5252281584031\n",
      "Reward for episode 10 is -102.24796280066296\n",
      "Reward for episode 11 is -139.87231554166542\n",
      "Reward for episode 12 is 83.62454598275161\n",
      "Reward for episode 13 is -147.56719920678654\n",
      "Reward for episode 14 is -237.9337072125185\n",
      "Reward for episode 15 is -264.37613398392864\n",
      "Reward for episode 16 is -403.60473431845145\n",
      "Reward for episode 17 is -480.5612094737238\n",
      "Reward for episode 18 is -292.66353812811496\n",
      "Reward for episode 19 is -331.1211833750699\n",
      "Reward for episode 20 is -635.5593732963673\n",
      "Reward for episode 21 is -668.276019853509\n",
      "Reward for episode 22 is -620.0157301257905\n",
      "Reward for episode 23 is -265.93444780580495\n",
      "Reward for episode 24 is -396.19326803958404\n",
      "Reward for episode 25 is -346.657431893547\n",
      "Reward for episode 26 is -139.3244499525972\n",
      "Reward for episode 27 is -448.02969027744825\n",
      "Reward for episode 28 is -354.8695245846954\n",
      "Reward for episode 29 is -355.5386127187785\n",
      "Reward for episode 30 is -154.31269815167286\n",
      "Reward for episode 31 is -72.41438494956036\n",
      "Reward for episode 32 is -136.81325886902545\n",
      "Reward for episode 33 is -158.87688891855683\n",
      "Reward for episode 34 is -112.1579486771733\n",
      "Reward for episode 35 is -210.55429395896647\n",
      "Reward for episode 36 is -124.3990857145306\n",
      "Reward for episode 37 is -107.35324036546137\n",
      "Reward for episode 38 is -123.57929620580204\n",
      "Reward for episode 39 is -132.9131056090901\n",
      "Reward for episode 40 is -99.05633470526732\n",
      "Reward for episode 41 is -102.66922512110013\n",
      "Reward for episode 42 is -97.6167478891239\n",
      "Reward for episode 43 is -107.67300325566822\n",
      "Reward for episode 44 is -109.57816016454252\n",
      "Reward for episode 45 is -140.31348553905158\n",
      "Reward for episode 46 is -124.60555388300023\n",
      "Reward for episode 47 is -78.1252777798248\n",
      "Reward for episode 48 is -89.93927772121478\n",
      "Reward for episode 49 is -82.37692462933116\n",
      "Reward for episode 50 is -113.90356025094094\n",
      "Reward for episode 51 is -69.94770837604334\n",
      "Reward for episode 52 is -114.19880598481123\n",
      "Reward for episode 53 is -93.09990619192445\n"
     ]
    }
   ],
   "source": [
    "state_num  = env.env.observation_space.shape[0]\n",
    "action_num = env.env.action_space.n    \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=120, activation='relu', input_dim=state_num))\n",
    "model.add(Dense(output_dim=60, activation='relu'))\n",
    "model.add(Dense(output_dim=action_num, activation='linear'))\n",
    "opt = RMSprop(lr=0.00025)\n",
    "model.compile(loss='mse', optimizer=opt) \n",
    "Reward_cum=np.zeros(1500)\n",
    "\n",
    "for episode in range(1500):\n",
    "    state = env.reset()\n",
    "    R = 0\n",
    "    state_num  = env.env.observation_space.shape[0]\n",
    "    action_num = env.env.action_space.n    \n",
    "   \n",
    "\n",
    "    while True:\n",
    "        \n",
    "        action = explore_Greedy(epsilon, predict_2(model,state))\n",
    "        \n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done: # terminal state\n",
    "            state2 = None\n",
    "            \n",
    "        sample_to_add = ((state,action, reward,state2))\n",
    "        \n",
    "        state = state2\n",
    "        epsilon = eps_reduce(epsilon,min_eps,max_eps,step)        \n",
    "        R += reward\n",
    "        step += 1\n",
    "        \n",
    "        mem_all.append(sample_to_add)\n",
    "        \n",
    "        if len(mem_all)>mem_allowed:\n",
    "            mem_all.pop(0)\n",
    "        \n",
    "        num = np.min([batch_size,len(mem_all)])\n",
    "        batch = random.sample(mem_all,num)\n",
    "\n",
    "        no_next_state = np.zeros(state_num)\n",
    "        states_now = np.array([iter[0] for iter in batch])\n",
    "        states_next = np.array([(no_next_state if iter2[3] is None else iter2[3]) for iter2 in batch])\n",
    "\n",
    "        predict_now = model.predict(states_now)\n",
    "        predict_next = model.predict(states_next)\n",
    "\n",
    "        x = np.zeros((len(batch),state_num))\n",
    "        y = np.zeros((len(batch),action_num))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            current_state = batch[i][0]\n",
    "            action_taken = batch[i][1]\n",
    "            reward_received = batch[i][2]\n",
    "            next_state = batch[i][3]\n",
    "\n",
    "            x[i] = current_state\n",
    "\n",
    "            target = predict_now[i]\n",
    "\n",
    "            if next_state is None:\n",
    "                target[action_taken] = reward_received\n",
    "            else:\n",
    "                target[action_taken] = reward_received+ Gamma*np.amax(predict_next[i])\n",
    "\n",
    "            y[i] = target\n",
    "            \n",
    "        model.fit(x,y,batch_size=batch_size,nb_epoch=1,verbose=0)\n",
    "        \n",
    "        #train_model(model,x,y)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "     \n",
    "    Reward_cum[episode]=R\n",
    "    print(\"Reward for episode %s is %s\" % (episode, R))\n",
    "    if episode > 98:\n",
    "        rm = np.mean(Reward_cum[episode-99:episode+1])\n",
    "        print(\"Rolling mean is %s\" %(rm))\n",
    "model.save(\"lunarlander_weights_pzou3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"lunarlander_weights_pzou3.h5\")\n",
    "\n",
    "R=np.zeros(100)\n",
    "env_ = gym.make(\"LunarLander-v2\")\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(output_dim=120, activation='relu', input_dim=state_num))\n",
    "model1.add(Dense(output_dim=60, activation='relu'))\n",
    "model1.add(Dense(output_dim=action_num, activation='linear'))\n",
    "opt = RMSprop(lr=0.00025)\n",
    "model1.compile(loss='mse', optimizer=opt) \n",
    "model1.load_weights(\"lunarlander_weights_pzou3.h5\")\n",
    "for j in range(100):    \n",
    "    s = env_.reset()\n",
    "    while True:\n",
    "        a=np.argmax(predict_2(model1,s))\n",
    "        s_, r, done, info = env_.step(a)\n",
    "        if done: # terminal state\n",
    "            s_ = None\n",
    "        s = s_\n",
    "        R[j] += r\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Test Reward:\", R[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training plot\n",
    "x1=range(1,1501)\n",
    "plt.plot(x1,Reward_cum)\n",
    "plt.xlabel('Training episode #')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training process with 120-60 structure')\n",
    "plt.show()    \n",
    "\n",
    "##testing plot\n",
    "x=range(1,101)\n",
    "avg_r=np.mean(R)\n",
    "plt.plot(x, R)\n",
    "plt.xlabel('Run #')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Testing process')\n",
    "plt.text(20,80,'Average reward:')\n",
    "plt.text(20,60,avg_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r=np.mean(R)\n",
    "print(avg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
